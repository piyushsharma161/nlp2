{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He is ::having a great Time ! to g 1, at 2 g the park time?',\n",
       " \"She, unlike most women, is a big player on the park's grass.\",\n",
       " \"she can't be going\"]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = [\"He is ::having a great Time ! to g 1, at 2 g the park time?\",\n",
    "       \"She, unlike most women, is a big player on the park's grass.\",\n",
    "       \"she can't be going\"]\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = ['He is ::having a great Time ! to g 1, at 2 g the park time?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['having', 'great', 'Time', 'park', 'time']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "# to remove symbol and numbers\n",
    "token1 = RegexpTokenizer(r'[a-zA-Z]+' r'\\w{2,}'  r\"[\\w']+\")\n",
    "token1.tokenize(''.join(txt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def text_process(mess):\n",
    "    # Remove punc\n",
    "    # Remove stop words\n",
    "    nopunc = [char for char in mess if char not in string.punctuation] \n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [char for char in nopunc.split() if char.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['::having', 'great', 'Time', '!', 'g', '1,', '2', 'g', 'park', 'time?']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_process(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "count_vec  = CountVectorizer(ngram_range=(1,1),analyzer=text_process,\n",
    "                             max_df=1.0,min_df=1,max_features=None, tokenizer=r'[a-zA-Z]+' r'\\w{2,}'  r\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train = count_vec.fit(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = count_vec.transform(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'great': 4, 'Time': 2, 'g': 3, '1': 0, '2': 1, 'park': 5, 'time': 6}\n"
     ]
    }
   ],
   "source": [
    "print(count_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', 'Time', 'g', 'great', 'park', 'time']\n"
     ]
    }
   ],
   "source": [
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'having': 8, 'great': 5, 'time': 17, 'park': 11, 'having great': 9, 'great time': 6, 'time park': 18, 'park time': 13, 'having great time': 10, 'great time park': 7, 'time park time': 19, 'unlike': 20, 'women': 23, 'big': 0, 'player': 14, 'grass': 4, 'unlike women': 21, 'women big': 24, 'big player': 1, 'player park': 15, 'park grass': 12, 'unlike women big': 22, 'women big player': 25, 'big player park': 2, 'player park grass': 16, 'going': 3}\n",
      "['big', 'big player', 'big player park', 'going', 'grass', 'great', 'great time', 'great time park', 'having', 'having great', 'having great time', 'park', 'park grass', 'park time', 'player', 'player park', 'player park grass', 'time', 'time park', 'time park time', 'unlike', 'unlike women', 'unlike women big', 'women', 'women big', 'women big player']\n"
     ]
    }
   ],
   "source": [
    "# N-gram\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec  = CountVectorizer(stop_words='english',ngram_range=(1,3),analyzer='word',\n",
    "                             max_df=1.0,min_df=1,max_features=None)\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "print(count_vec.vocabulary_)\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_fitted = tf.fit(txt1)\n",
    "txt_transformed = txt_fitted.transform(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his': 0, 'smile': 5, 'was': 6, 'not': 1, 'perfect': 2, 'she': 4, 'sang': 3}\n"
     ]
    }
   ],
   "source": [
    "print(txt_fitted.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf  = tf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his': 1.4054651081081644, 'not': 1.0, 'perfect': 1.4054651081081644, 'sang': 2.09861228866811, 'she': 2.09861228866811, 'smile': 1.4054651081081644, 'was': 1.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "print(dict(zip(txt_fitted.get_feature_names(), idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>his</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perfect</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sang</td>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>she</td>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>smile</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>was</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token    weight\n",
       "0      his  1.405465\n",
       "1      not  1.000000\n",
       "2  perfect  1.405465\n",
       "3     sang  2.098612\n",
       "4      she  2.098612\n",
       "5    smile  1.405465\n",
       "6      was  1.405465"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rr = pd.DataFrame(list(zip(txt_fitted.get_feature_names(), idf)), columns=('token','weight'))\n",
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8f64643400>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x='token', y='weight', data=rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1 = tf.transform(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40546511, 4.        , 1.40546511, 2.09861229, 2.09861229,\n",
       "       1.40546511, 1.40546511])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new1.max(axis=0).toarray().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "holdout = pd.read_csv('test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '= From RfC == \\n\\n The title is fine as it is.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'= From RfC ==  The title is fine as it is.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"\\n\", '', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].map(lambda x: re.sub('\\\\n', ' ', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '[User11] piyush is not that gud, comment by User Amit, he is with us from last 5 years'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[User11] piyush is not that gud, comment by User Amit, he is with us from last 5 years'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"\\[\\[User.*\",'',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"Hello piyush. It's good to see you. Thanks for buying this book\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello piyush.', \"It's good to see you.\", 'Thanks for buying this book']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "#tokenzier = nltk.data.load('tokenziers/punkt/PY3/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'piyush', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Thanks', 'for', 'buying', 'this', 'book']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'can', \"'\", 't']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"i can't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', \"can't\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"i can't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', \"can't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[\\w']+\", \"i can't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['piyu', 'piyu']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'piyu', \"piyush piyu can't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', \"can't\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(\"i can't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'can', 't']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "tokenizer.tokenize(\"i can't 100 1 9 . !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtd = {'id':[1,2,3], 'msg':[\"He is ::having a great Time ! to g 1, at 2 g the park time?\",\n",
    "       \"She, unlike most women, is a  ty big player on the park's grass.\",\n",
    "       \"she can't be going\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(txtd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>He is ::having a great Time ! to g 1, at 2 g t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>She, unlike most women, is a  ty big player on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>she can't be going</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                msg\n",
       "0   1  He is ::having a great Time ! to g 1, at 2 g t...\n",
       "1   2  She, unlike most women, is a  ty big player on...\n",
       "2   3                                 she can't be going"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['having', 'great', 'Time', 'park', 'time']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "# to remove symbol and numbers\n",
    "token1 = RegexpTokenizer(r'[a-zA-Z]+' r'\\w{2,}'  r\"[\\w']+\")\n",
    "token1.tokenize(''.join(txt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "token1 = RegexpTokenizer(r'[a-zA-Z]+' r'\\w{1,}')\n",
    "def text_process(mess):\n",
    "    # Remove punc\n",
    "    # Remove stop words\n",
    "    nopunc =  [char for char in ''.join(mess).split() if char.lower() not in stopwords.words('english')]\n",
    "    #nopunc = ''.join(nopunc)\n",
    "    nopunc = [char for char in nopunc if char not in string.punctuation] \n",
    "  \n",
    "    \n",
    "    return token1.tokenize(' '.join(nopunc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [having, great, Time, park, time]\n",
       "Name: msg, dtype: object"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# He is ::having a great Time ! to g 1, at 2 g the park time?\n",
    "df['msg'][0:1].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "count_vec  = CountVectorizer(ngram_range=(1,1),analyzer=text_process,\n",
    "                             max_df=1.0,min_df=1,max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'having': 7, 'great': 6, 'Time': 1, 'park': 8, 'time': 10, 'She': 0, 'unlike': 12, 'women': 13, 'ty': 11, 'big': 2, 'player': 9, 'grass': 5, 'can': 3, 'going': 4}\n",
      "['She', 'Time', 'big', 'can', 'going', 'grass', 'great', 'having', 'park', 'player', 'time', 'ty', 'unlike', 'women']\n"
     ]
    }
   ],
   "source": [
    "count_train = count_vec.fit(df['msg'])\n",
    "bag_of_words = count_vec.transform(df['msg'])\n",
    "print(count_vec.vocabulary_)\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [great, Time, g, 1, 2, g, park, time]\n",
       "Name: msg, dtype: object"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['msg'][0:1].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['having', 'great', 'Time', 'park', 'time']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1.tokenize(''.join(df['msg'][0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He is ::having a great Time ! to g 1, at 2 g the park time?',\n",
       " \"She, unlike most women, is a  ty big player on the park's grass.\",\n",
       " \"she can't be going\"]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[char for char in df['msg'] if char.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'is',\n",
       " '::having',\n",
       " 'a',\n",
       " 'great',\n",
       " 'Time',\n",
       " '!',\n",
       " 'to',\n",
       " 'g',\n",
       " '1,',\n",
       " 'at',\n",
       " '2',\n",
       " 'g',\n",
       " 'the',\n",
       " 'park',\n",
       " 'time?']"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(df['msg'][0:1]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
